---
title: "Homework 3 Allen Zhu"
output:
  html_document:
    df_print: paged
---

# Lab Section

In this lab, we will go over regularization, classification and performance metrics. We will be using the caret package in R. https://topepo.github.io/caret/train-models-by-tag.html

# Perfomance Metrics 

## K- fold cross validatation - Resampling method

Randomly split the training data into k folds. If you specify 10 folds, then you split the data into 10 partitions. Train the model on 9 of those partitions, and test your model on the 10th partition. Iterate through until every partition has been held out. 

A smaller k is more biased, but a larger k can be very variable. 

## Bootstrapping - Resampling method

Sample with replacement. Some samples may be represented several times within the boostrap sample, while others may not be represented at all. The samples that are not selected are called out of bag samples. 

Boostrap error rates usually have less uncertainty than k-fold cross validation, but higher bias. 

## Error

Deviation of the observed value to the true value (population mean)

## Residual 

Deviation of the observed value to the estimated value (sample mean)
$$residual=y_i - \hat{y_i}$$
where $\hat{y_i}$ is the estimated value

## Mean Squared Error (MSE)

$$MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2$$

## Root Mean Squared Error (RMSE)
Same units as original data.

$$RMSE=\sqrt{MSE}$$

## R^2
Proportion of information explained by the model. It is a measure of correlation, not accuracy. 
$$1-RSS/TSS$$ 

## L2 regularization : Ridge regression. Regularize by adding the sum of the coefficients, squared, to the function. 

$$Ridge Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p(w_j)^2$$

## L1 regularization : Lasso Regression. Regularize by adding the sum of the absolute value of the coefficients to the model. Coefficient estimates may be pushed to zero -- Lasso can perform variable selection

$$Lasso Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p|w_j|$$

## Sensitivity or True Positive Rate

TP = True Positives
TN = True Negatives
FP = False Positives - Type I error
FN =  False Negatives - Type II error
N = actual negative samples
P = actual positive samples

$$TPR=TP/(TP + FN)$$

## Specificity or True Negative Rate

$$TNR=TN/(TN + FP)$$

## Receiver Operating Characteristics (ROC)

Plot of True Positive Rate (sensitivity) against False Positive Rate, or plots the True Positive Rate (sensitivity) against specificity. 

Either way, a good ROC curves up through the left corner, and has a large area underneath. 

## Area under ROC curve (AUC)

The area underneath the ROC curve

## Logistic function:

$$P(X)=e^{w_0 + w_1X}/{1+e^{w_0+w_1X}}$$

\newpage

### The broad steps of Machine learning in R. 

1. Split the data into training and test. Set test aside. 

2. Fit a good model to the training data. This includes using bootstapping, cross validation etc. to resample the training data and fit a good model.

3. Visualize if your model learned on the training data by looking at ROC curve and AUC.

4. Test how your model performs on the test data. 

### Broad steps for choosing between models according to Max Kuhn and Kjell Johnson

1. Start with several models that are the least interpretable and the most flexible, like boosted trees and svms. These models are the often the most accurate.

2. Investigate simpler models that are less opaque, like partial least squares, generalized additive models, or naive bayes models.

3. Consider using the simplest model that reasonable approximates the performance of more complex models

\newpage

```{r, include=FALSE}
library(caret)
library(ROCR)
library(pROC)
library(MASS)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(ggfortify)
library(glmnet)
library(tidyverse)
library(e1071)


```



```{r include=FALSE}
#Split data into training and test set

train_size <- floor(0.75 * nrow(airquality))
set.seed(543)
train_pos <- sample(seq_len(nrow(airquality)), size = train_size)
train_regression <- airquality[train_pos,-c(1,2)]
test_regression <- airquality[-train_pos,-c(1,2)]

dim(train_regression)
dim(test_regression)
```

## Resampling in R
```{r include=FALSE}
# ?trainControl
```

## Ridge Regression

$$Ridge Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p(w_j)^2$$
2. Create and train model 
```{r include=FALSE}
ctrl =  trainControl(method = "boot", 15)

Ridge_regression <- train(Temp ~ Wind + Month, data = train_regression,
                          method = 'ridge', trControl= ctrl) 
```

```{r include=FALSE}
Ridge_regression 

#Choose minimum lambda
```

Examine the residuals 
```{r include=FALSE}
ridge_test_pred <- predict(Ridge_regression, newdata = test_regression)

#plot the predicted values vs the observed values
plot_ridge_test_pred <- data.frame(Temp_test_pred = ridge_test_pred, 
                                   Observed_Temp = test_regression$Temp)
ggplot(data = plot_ridge_test_pred) +
  geom_point(aes(x=Observed_Temp, y = Temp_test_pred)) + 
  ggtitle("True Temp Value vs Predicted Temp Value Ridge Regression") +
  theme_bw()

#median residual value should be close to zero
median(resid(Ridge_regression))
```


# Homework

## Lasso

$$Lasso Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p|w_j|$$
2. Create and train model 
```{r}
ctrl =  trainControl(method = "boot", 15)

Lasso_regression <- train(Temp ~ Wind + Month, data = train_regression,
                          method = 'lasso', trControl= ctrl) 

```

Examine the residuals 
```{r}
lasso_test_pred <- predict(Lasso_regression, newdata = test_regression)

#plot the predicted values vs the observed values
plot_lasso_test_pred <- data.frame(Temp_test_pred = lasso_test_pred, 
                                   Observed_Temp = test_regression$Temp)
ggplot(data = plot_lasso_test_pred) +
  geom_point(aes(x=Observed_Temp, y = Temp_test_pred)) + 
  ggtitle("True Temp Value vs Predicted Temp Value Ridge Regression") +
  theme_bw()

#median residual value should be close to zero
median(resid(Lasso_regression))
```


# Classification

1. Split into training and test set 
```{r include=FALSE}
data(iris)

#split into training and test set 
train_size <- floor(0.75 * nrow(iris))
set.seed(543)
train_pos <- sample(seq_len(nrow(iris)), size = train_size)
train_classifier <- iris[train_pos,]
test_classifier <- iris[-train_pos,]

dim(train_classifier)
dim(test_classifier)
```


## Linear Discriminant analysis

* Good for well separated classes, more stable with small n than logistic regression, and good for more than 2 response classes. 
* LDA assumes a normal distribution with a class specific mean and common variance. 

Let's see if our data follows the assumptions of LDA. 
```{r include=FALSE}
slength <- ggplot(data = iris, aes(x = Sepal.Length, fill = Species)) + 
  geom_histogram(position="identity", alpha=0.5, bins= 25)  +
  theme_bw()
swidth <- ggplot(data = iris, aes(x = Sepal.Width, fill = Species)) + 
  geom_histogram(position="identity", alpha=0.5, bins= 25) +
  theme_bw()
plength <- ggplot(data = iris, aes(x = Petal.Length, fill = Species)) + 
  geom_histogram(position="identity", alpha=0.5, bins= 25) +
  theme_bw()
pwidth <- ggplot(data = iris, aes(x = Petal.Width, fill = Species)) + 
  geom_histogram(position="identity", alpha=0.5, bins= 25) +
  theme_bw()

grid.arrange(slength, swidth, plength, pwidth)
```

```{r}
LDA <- lda(Species~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, 
           data= train_classifier, cv= T)
#cv  is cross validation
```

```{r include=FALSE}
LDA
```

4. Test model on test set 
```{r include=FALSE}
#predict the species of the test data
LDA_predict <- predict(LDA, newdata=test_classifier)
confusionMatrix(LDA_predict$class, reference = test_classifier$Species)
```


```{r include=FALSE}
# save the predictions in a new variable
predictions <- as.data.frame(LDA_predict$posterior) %>% 
  rownames_to_column("idx")

test_classifier <- test_classifier %>% 
  rownames_to_column("idx")

predictions_actual <- full_join(predictions,test_classifier, by = "idx" )

# choose the two classes we want to compare, setosa and versicolor
set_vers_true_labels <- predictions_actual %>% 
  filter(Species %in% c("setosa", "versicolor")) %>% 
  mutate(Species = as.character(Species)) 
  
#make dataframe of the prediction and the label
pred_label <- data.frame(prediction = set_vers_true_labels$setosa,
                         label = set_vers_true_labels$Species)

ggplot(pred_label, aes(x = 1:24, y = prediction, color = label))+
  geom_point()

pred <- prediction(set_vers_true_labels$setosa, set_vers_true_labels$Species, 
label.ordering = c("versicolor", "setosa")) 

perf <- performance(pred,"tpr","fpr")
plot(perf)
```


## Logistic Regression

$logodds_i=B_0 + B_1X_{i1}$

Here, the log odds represents the log odds of $Y_i$ being 0 or 1. 

Where $logodds$ is the dependent variable, and $X_i$ is the independent variable. $B_{number}$ are the parameters to fit. 

Logistic Regression assumes a linear relationship between the $logodds$ and $X$.

To convert from logodds, a not intuitive quantity, to odds, a more intuitive quantity, we use this non-linear equation: 

$odds_i=e^{logodds_{i}}$
or 
$odds_i=e^{B_0 + B_1X_{i1}}$

Odds is defined as the probability that the event will occur divided by the probability that the event will not occur.

Now we convert from odds to probability.

The probability that an event will occur is the fraction of times you expect to see that event in many trials. Probabilities always range between 0 and 1.

To convert from odds to a probability, divide the odds by one plus the odds. So to convert odds of 1/9 to a probability, divide 1/9 by 10/9 to obtain the probability of 0.10

$P=odds/(odds+1)$


## Logistic Regression implementation

* Y=1 is the probability of the event occuring.
* Independent variables should not be correlated.
* Log odds and independent variables should be linearly correlated.

2. Train and fit model 
```{r include=FALSE}
data(iris)

#split into training and test set 
train_size <- floor(0.75 * nrow(iris))
set.seed(543)
train_pos <- sample(seq_len(nrow(iris)), size = train_size)
train_classifier <- iris[train_pos,]
test_classifier <- iris[-train_pos,]


dim(train_classifier)
dim(test_classifier)
#only look at two classes 
train_classifier_log <- train_classifier[c(which(train_classifier$Species == "setosa"),
                                           which(train_classifier$Species == "versicolor")),]
test_classifier_log <- test_classifier[c(which(test_classifier$Species == "setosa"), 
                                         which(test_classifier$Species == "versicolor")),]

train_classifier_log$Species <- factor(train_classifier_log$Species)
test_classifier_log$Species <- factor(test_classifier_log$Species)

ctrl <- trainControl(method = "repeatedcv", repeats = 15,classProbs = T,
                     savePredictions = T)

#create model. logistic regression is a bionomial general linear model. 
#predict species based on sepal length
logistic_regression <- train(Species~ Sepal.Length, data = train_classifier_log, 
                             method = "glm", family= "binomial", trControl = ctrl)

train_classifier_log$Species

```


```{r include=FALSE}
logistic_regression
```


```{r include=FALSE}
summary(logistic_regression)
```

3. Visualize ROC curve 
```{r include=FALSE}
plot(x = roc(predictor = logistic_regression$pred$setosa,
             response = logistic_regression$pred$obs)$specificities, 
     y = roc(predictor = logistic_regression$pred$setosa, 
             response = logistic_regression$pred$obs)$sensitivities,
     col= "blue", xlim = c(1, 0), type ="l", ylab = "Sensitivity",
     xlab = "Specificity")
legend("bottomright", legend = paste("setosa v versicolor --", 
                                     roc(predictor = logistic_regression$pred$setosa,
                                         response = logistic_regression$pred$obs)$auc
, sep = ""), col = c("blue"), fill = c("blue"))
```

4. Test on an independent set
```{r include=FALSE}
#predict iris species using Sepal legth
logistic_regression_predict_class <- predict(logistic_regression, newdata = test_classifier_log)

#confusion matrix
confusionMatrix(logistic_regression_predict_class, 
                reference = test_classifier_log$Species)

```

Check if log odds and independent variables are linearly correlated
```{r include=FALSE}


logistic_regression_predict <- predict(logistic_regression, 
                                       newdata = test_classifier_log, type = "prob")

odds_species1 <- logistic_regression_predict[,1] / (1 - logistic_regression_predict[,1])

# To convert from a probability to odds, divide the probability by one minus that probability. So if the probability is 10% or 0.10 , then the odds are 0.1/0.9 or ‘1 to 9’ 

odds_species1 <- logistic_regression_predict[,1] / (1 - logistic_regression_predict[,1])
log_odds_species1 <- log(odds_species1)
cor.test(log_odds_species1, test_classifier_log$Sepal.Length)
plot(log_odds_species1, test_classifier_log$Sepal.Length)
```

Look deeper at the logistic regression 
```{r include=FALSE}
logistic_predict_prob <- predict(logistic_regression,
                                 newdata = test_classifier_log, type="prob")

logistic_pred_prob_plot <- data.frame(Species_pred = logistic_predict_prob, Sepal.Length  = test_classifier_log$Sepal.Length) 

test_classifier_log$Species <- as.numeric(test_classifier_log$Species) -1

ggplot(data = test_classifier_log) +
  geom_point(aes(x=Sepal.Length, y = Species)) + 
  geom_line(data = logistic_pred_prob_plot, aes(x = Sepal.Length, 
                                                y = Species_pred.setosa, col =  "setosa"))+
  geom_line(data = logistic_pred_prob_plot, aes(x = Sepal.Length,
                                                y = Species_pred.versicolor, col = "versicolor"))+
  ggtitle("Probabilities for classifying species")

```

#Homework:

1. Use the Breast Cancer dataset from the mlbench package, and predict whether the cancer is malignant or benign using one of the algorithms we learned about in class. Give some rationale as to why you chose this algorithm. Plot ROC curves, and confusion matrices. If you are choosing a hyperparameter like K or lambda, explain how and why you chose it. 

I selected a logistic regression to predict malignancy on the Breast Cancer dataset because it was testing between two classes: benign and malignant. Furthermore since the dependent variable is binary, we can then utilize the independent variables to make predictive analysis to make probability estimations. 

```{r}
library(mlbench)
```


```{r}
data(BreastCancer)


str(BreastCancer)

```


```{r}
# Train/Test set split 

BC_size <- floor(0.75 * nrow(BreastCancer))
set.seed(15)
BC_pos <- sample(seq_len(nrow(BreastCancer)), size = BC_size)
train_classifier <- BreastCancer[BC_pos,]
test_classifier <- BreastCancer[-BC_pos,]

dim(train_classifier)
dim(test_classifier)




```




```{r ignore = TRUE}
#Only 2 available.
train_classifier_log <- train_classifier[c(which(train_classifier$Class == "benign"),
                                           which(train_classifier$Class == "malignant")),]


test_classifier_log <- test_classifier[c(which(test_classifier$Class == "benign"), 
                                         which(test_classifier$Class == "malignant")),]



ctrl <- trainControl(method = "repeatedcv", repeats = 15,classProbs = T,
                     savePredictions = T)

#Initially attempted to create a logistic regression model evaluating all the variables at once, and ended up with an error where my Pr(Z>|z|) values were all wonky, so I ran each variable separately


#Bare.nuclei had na values, so I created a 'cleaned' training dataset that omitted all the nas. The following set of logistic regressions were all done on this 'cleaned' data set to promote consistency.
train_clean <- na.omit(train_classifier_log)
test_clean <- na.omit(test_classifier_log)

dim(train_classifier_log)
dim(test_classifier_log)

dim(train_clean)


# After running into an error further down the code, where the predictor curves required numerical inputs. The three tested variables were coerced into a numeric format.


train_clean$Bare.nuclei <- as.numeric(as.character(train_clean$Bare.nuclei))
train_clean$Mitoses <- as.numeric(as.character(train_clean$Mitoses))
train_clean$Normal.nucleoli <- as.numeric(as.character(train_clean$Normal.nucleoli))



test_clean$Bare.nuclei <- as.numeric(as.character(test_clean$Bare.nuclei))
test_clean$Mitoses <- as.numeric(as.character(test_clean$Mitoses))
test_clean$Normal.nucleoli <- as.numeric(as.character(test_clean$Normal.nucleoli))

```


```{r}
# Individual logistic regressions for all the variables.

BC_THC <- train(Class ~ Cl.thickness, data = train_clean, 
                             method = "glm", family= "binomial", trControl = ctrl)

summary(BC_THC)

BC_CSize <- train(Class ~ Cell.size, data = train_clean, 
                             method = "glm", family= "binomial", trControl = ctrl)
summary(BC_CSize)


BC_CShape <- train(Class ~ Cell.shape, data = train_clean, 
                             method = "glm", family= "binomial", trControl = ctrl)
summary(BC_CShape)


BC_MA <- train(Class ~ Marg.adhesion, data = train_clean, 
                             method = "glm", family= "binomial", trControl = ctrl)
summary(BC_MA)


BC_ECC <- train(Class ~ Epith.c.size, data = train_clean, 
                             method = "glm", family= "binomial", trControl = ctrl)
summary(BC_ECC)


BC_BN <- train(Class ~ Bare.nuclei, data = train_clean, 
                             method = "glm", family= "binomial", trControl = ctrl)
summary(BC_BN)


BC_BC <- train(Class ~ Bl.cromatin, data = train_clean, 
                             method = "glm", family= "binomial", trControl = ctrl)

BC_NN <- train(Class ~  Normal.nucleoli, data = train_clean, 
                             method = "glm", family= "binomial", trControl = ctrl)
summary(BC_NN)


BC_M <- train(Class ~  Mitoses, data = train_clean, 
                             method = "glm", family= "binomial", trControl = ctrl)
summary(BC_M)


```




```{r }
# After evaluating the logistic regressions, it seems that the variables that have a significant effect are Bare.nuclei, Normal.nucleoli, and Mitoses. A logistic regression was then run on just those three variables together. Unfortunately this resulted in a warning that stated the model reached reached what is assumed to be a perfect explanation, suggesting overfitting (the specific error is the following: glm.fit: fitted probabilities numerically 0 or 1 occurred). Any two of the three listed variables added together yielded the same result, so the three were tested separately to measure their effectiveness in predicting the cancer class. 


# BC_Log <- train(Class ~ Bare.nuclei + Normal.nucleoli + Mitoses, data = train_clean, 
                              # method = "glm", family= "binomial", trControl = ctrl)


# summary(BC_Log)

```




```{r }
# Roc Curve for Bare Nuclei 

plot(x = roc(predictor = BC_BN$pred$malignant,
             response = BC_BN$pred$obs)$specificities, 
     y = roc(predictor = BC_BN$pred$malignant,
             response = BC_BN$pred$obs)$sensitivities,
     col= "blue", xlim = c(1, 0), type ="l", ylab = "Sensitivity",
     xlab = "Specificity")
legend("bottomright", legend = paste("malignant v benign --", 
                                     roc(predictor = BC_BN$pred$malignant,
                                         response = BC_BN$pred$obs)$auc
, sep = ""), col = c("blue"), fill = c("blue"))

```


```{r}
# Confusion matrix for bare nuclei and cancer malignancy


#predict cancer class using Bare Nuclei
BN_predict_class <- predict(BC_BN, newdata = test_clean)

#confusion matrix
confusionMatrix(BN_predict_class, 
                reference = test_clean$Class)
```


```{r}
# Checking for correlation between log odds and bare nuclei

BN_predict <- predict(BC_BN, newdata = test_clean, type = "prob")

# To convert from a probability to odds, divide the probability by one minus that probability. So if the probability is 10% or 0.10 , then the odds are 0.1/0.9 or ‘1 to 9’ 

bnuclei <- BN_predict[,1] / (1 - BN_predict[,1])
log_bnuclei <- log(bnuclei)
cor.test(log_bnuclei, test_clean$Bare.nuclei)
plot(log_bnuclei, test_clean$Bare.nuclei)
```

```{r}
# Create a new column in the data set where the classes were turned into numbers, and subtracted by 1, so that the probability plot can be made without actually changing the original class column

test_clean <- mutate(test_clean, Classifications = as.numeric(test_clean$Class) -1)

#Probability curve for bare nuclei predicting breast cancer malignancy.

BN_predict_prob <- predict(BC_BN, newdata = test_clean, type="prob")

Bare_nuclei_pred_prob_plot <- data.frame(Class_pred = BN_predict_prob, Bare.nuclei  = test_clean$Bare.nuclei) 


ggplot(data = test_clean) +
  geom_point(aes(x=Bare.nuclei, y = Classifications)) + 
  geom_line(data = Bare_nuclei_pred_prob_plot, aes(x = Bare.nuclei, 
                                                y = Class_pred.benign, col =  "benign"))+
  geom_line(data = Bare_nuclei_pred_prob_plot, aes(x = Bare.nuclei,
                                                y = Class_pred.malignant, col = "malignant"))+
  ggtitle("Probabilities for Cancer Class Based on Bare Nuclei")

```



```{r}
#ROC Curve for normal nucleoli 

plot(x = roc(predictor = BC_NN$pred$malignant,
             response = BC_NN$pred$obs)$specificities, 
     y = roc(predictor = BC_NN$pred$malignant, 
             response = BC_NN$pred$obs)$sensitivities,
     col= "blue", xlim = c(1, 0), type ="l", ylab = "Sensitivity",
     xlab = "Specificity")
legend("bottomright", legend = paste("malignant v benign --", 
                                     roc(predictor = BC_NN$pred$malignant,
                                         response = BC_NN$pred$obs)$auc
, sep = ""), col = c("blue"), fill = c("blue"))
```


```{r}
#Confusion matrix for normal nucleoli on cancer malignancy

#predict iris species using Sepal legth
NN_predict_class <- predict(BC_NN, newdata = test_clean)

#confusion matrix
confusionMatrix(NN_predict_class, 
                reference = test_clean$Class)
```

```{r}
#Correlation check on log odds and normal nucleoli


NN_predict <- predict(BC_NN, newdata = test_clean, type = "prob")

# To convert from a probability to odds, divide the probability by one minus that probability. So if the probability is 10% or 0.10 , then the odds are 0.1/0.9 or ‘1 to 9’ 

Norm_nucleoli <- NN_predict[,1] / (1 - NN_predict[,1])
log_normnuc <- log(Norm_nucleoli)
cor.test(log_normnuc, test_clean$Normal.nucleoli)
plot(log_normnuc, test_clean$Normal.nucleoli)
```


```{r}
# Probability plot for normal nucleoli and cancer malignancy

NN_predict_prob <- predict(BC_NN,  newdata = test_clean, type="prob")

Normal_nucleoli_pred_prob_plot <- data.frame(Class_pred = NN_predict_prob, Normal.nucleoli  = test_clean$Normal.nucleoli) 


ggplot(data = test_clean) +
  geom_point(aes(x=Normal.nucleoli, y = Classifications)) + 
  geom_line(data = Normal_nucleoli_pred_prob_plot, aes(x = Normal.nucleoli, 
                                                y = Class_pred.benign, col =  "benign"))+
  geom_line(data = Normal_nucleoli_pred_prob_plot, aes(x = Normal.nucleoli,
                                                y = Class_pred.malignant, col = "malignant"))+
  ggtitle("Probabilities for classifying species")

```



```{r}
#ROC curve for mitoses and cancer malignancy 

plot(x = roc(predictor = BC_M$pred$malignant,
             response = BC_M$pred$obs)$specificities, 
     y = roc(predictor = BC_M$pred$malignant, 
             response = BC_M$pred$obs)$sensitivities,
     col= "blue", xlim = c(1, 0), type ="l", ylab = "Sensitivity",
     xlab = "Specificity")
legend("bottomright", legend = paste("malignant v benign --", 
                                     roc(predictor = BC_M$pred$malignant,
                                         response = BC_M$pred$obs)$auc
, sep = ""), col = c("blue"), fill = c("blue"))

```








```{r}
#Confusion matri between mitoses class and cancer malignancy 
M_predict_class <- predict(BC_M, newdata = test_clean)

#confusion matrix
confusionMatrix(M_predict_class, 
                reference = test_clean$Class)
```

```{r}
# Correlation test between log odds and mitoses

M_predict <- predict(BC_M,  newdata = test_clean, type = "prob")

# To convert from a probability to odds, divide the probability by one minus that probability. So if the probability is 10% or 0.10 , then the odds are 0.1/0.9 or ‘1 to 9’ 

nonlog_mito <- M_predict[,1] / (1 - M_predict[,1])
log_mito <- log(nonlog_mito)
cor.test(log_mito, test_clean$Mitoses)
plot(log_mito, test_clean$Mitoses)




```

```{r}
# Probability plots of mitoses and cancer malignancy
M_predict_prob <- predict(BC_M,  newdata = test_clean, type="prob")

Mitoses_pred_prob_plot <- data.frame(Class_pred = M_predict_prob, Mitoses  = test_clean$Mitoses) 


ggplot(data = test_clean) +
  geom_point(aes(x=Mitoses, y = Classifications)) + 
  geom_line(data = Mitoses_pred_prob_plot, aes(x = Mitoses, 
                                                y = Class_pred.benign, col =  "benign"))+
  geom_line(data = Mitoses_pred_prob_plot, aes(x = Mitoses,
                                                y = Class_pred.malignant, col = "malignant"))+
  ggtitle("Probabilities for classifying species")

```












References: 
https://sebastianraschka.com/Articles/2014_python_lda.html

https://towardsdatascience.com/building-a-multiple-linear-regression-model-and-assumptions-of-linear-regression-a-z-9769a6a0de42

http://www.statisticssolutions.com/wp-content/uploads/wp-post-to-pdf-enhanced-cache/1/assumptions-of-logistic-regression.pdf

https://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/  , https://sebastianraschka.com/Articles/2014_python_lda.html


Other cool sites: 
https://www.countbayesie.com/blog/2019/6/12/logistic-regression-from-bayes-theorem
https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/